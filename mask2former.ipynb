{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation,Mask2FormerConfig,SwinConfig,Mask2FormerImageProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from utils.dataset_utils import CadisDataset,Cataract101Dataset\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 8478), started 0:27:45 ago. (Use '!kill 8478' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4351e9cb8c5ce0ce\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4351e9cb8c5ce0ce\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_dir=\"outputs/\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir outputs/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Datasets and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3584, 674, 540, 84, 614, 85)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cataract_101_dataset=Cataract101Dataset(root_folder=\"data/cataract-101\", split=\"train\")\n",
    "total_size=len(cataract_101_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "generator1=torch.Generator().manual_seed(42)\n",
    "cataract_101_train_dataset, cataract_101_val_dataset, cataract_101_test_dataset = random_split(cataract_101_dataset, [train_size, val_size, test_size],generator=generator1)\n",
    "\n",
    "cadis_train_dataset = CadisDataset(root_folder=\"data/cadis\", split=\"train\")\n",
    "cadis_val_dataset = CadisDataset(root_folder=\"data/cadis\", split=\"val\")\n",
    "cadis_test_dataset = CadisDataset(root_folder=\"data/cadis\", split=\"test\")\n",
    "len(cadis_train_dataset),len(cataract_101_train_dataset),len(cadis_val_dataset),len(cataract_101_val_dataset),len(cadis_test_dataset),len(cataract_101_test_dataset)\n",
    "\n",
    "# TODO: merge A+B, rand(A)+B train and val sets: fully_merged_train_dataset, fully_merged_val_dataset, replayA_B_train_dataset, replayA_B_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing The M2F Model, Configs and The Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinBackbone were not initialized from the model checkpoint at microsoft/swin-large-patch4-window12-384-in22k and are newly initialized: ['swin.hidden_states_norms.stage4.bias', 'swin.hidden_states_norms.stage4.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ONLY BACKBONE PRETRAINED ON IN22K -> probable baseline\n",
    "config=Mask2FormerConfig()\n",
    "config.use_pretrained_backbone=True\n",
    "config.backbone_config=None\n",
    "config.backbone=\"microsoft/swin-large-patch4-window12-384-in22k\"\n",
    "num_classes=len(cadis_train_dataset.categories) # including background\n",
    "config.num_labels=num_classes-1\n",
    "config.backbone,config.backbone_config,config.use_pretrained_backbone\n",
    "image_processor=Mask2FormerImageProcessor(ignore_index=255, reduce_labels=True)\n",
    "model=Mask2FormerForUniversalSegmentation(config)\n",
    "\n",
    "# FULL MODEL PRETRAINED ON ADE20K -> We might also use this as baseline\n",
    "\"\"\"num_classes=len(cadis_train_dataset.categories) # including background\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-large-ade-semantic\",\n",
    "                    ignore_index=255, reduce_labels=True)\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-large-ade-semantic\",\n",
    "                num_labels=num_classes-1,ignore_mismatched_sizes=True)\"\"\"\n",
    "\n",
    "# MODEL PREVIOUSLY TRAINED ON ONE OF OUR DATASETS\n",
    "\"\"\"\n",
    "num_classes=??? # including background\n",
    "image_processor = AutoImageProcessor.from_pretrained(<location_to_trained_model_dir>,\n",
    "                    ignore_index=255, reduce_labels=True)\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(<location_to_trained_model_dir>,\n",
    "                num_labels=num_classes-1,ignore_mismatched_sizes=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "learning_rate_multiplier=0.1\n",
    "backbone_lr=LEARNING_RATE*learning_rate_multiplier\n",
    "weight_decay=0.5\n",
    "#dice = Dice(average='micro')\n",
    "\n",
    "#lambda_CE=5.0\n",
    "#lambda_dice=5.0\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "encoder_params=[param for name, param in model.named_parameters() if name.startswith(\"model.pixel_level_module.encoder\")]\n",
    "decoder_params=[param for name, param in model.named_parameters() if name.startswith(\"model.pixel_level_module.decoder\")]\n",
    "transformer_params=[param for name, param in model.named_parameters() if name.startswith(\"model.transformer_module\")]\n",
    "optimizer = optim.AdamW([{'params': encoder_params, 'lr': backbone_lr},\\\n",
    "                         {'params': decoder_params}, \\\n",
    "                            {'params':transformer_params}], \\\n",
    "                                lr=LEARNING_RATE,weight_decay=weight_decay)\n",
    "\n",
    "scheduler = optim.lr_scheduler.PolynomialLR(optimizer,total_iters=NUM_EPOCHS,power=0.9)\n",
    "\"\"\"\n",
    "or \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = MultiStepLR(\n",
    "        optimizer, milestones=50, gamma=0.1, verbose=True\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "#CE_weight = torch.ones(num_classes)*2.0\n",
    "#CE_weight[0] = 0.1\n",
    "\n",
    "# Dataloading\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 16\n",
    "SHUFFLE = True\n",
    "DROP_LAST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: model pretrained on ADE20K, finetune on only A, test A and B\n",
    "train_loader_A = DataLoader(cadis_train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=N_WORKERS, drop_last=DROP_LAST, pin_memory=True)\n",
    "val_loader_A=DataLoader(cadis_val_dataset,batch_size=2,shuffle=False,num_workers=N_WORKERS,drop_last=DROP_LAST)\n",
    "test_loader_A=DataLoader(cadis_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "test_loader_B=DataLoader(cataract_101_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "model_name=\"m2f_cadis\"\n",
    "val_dataset_name=\"cadis\"\n",
    "\n",
    "# Option 2: model pretrained on A, finetune on  A+B, test A and B\n",
    "\"\"\"\n",
    "train_loader_fully_merged = DataLoader(fully_merged_train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=N_WORKERS, drop_last=DROP_LAST, pin_memory=True)\n",
    "val_loader_fully_merged=DataLoader(fully_merged_val_dataset,batch_size=2,shuffle=False,num_workers=N_WORKERS,drop_last=DROP_LAST)\n",
    "test_loader_A=DataLoader(cadis_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "test_loader_B=DataLoader(cataract_101_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "model_name=\"m2f_cadis+cataract101\"\n",
    "val_dataset_name=\"cadis+cataract101\"\n",
    "\"\"\"\n",
    "\n",
    "# Option 3: model pretrained on A, finetune on B, test A and B\n",
    "\"\"\"\n",
    "train_loader_B = DataLoader(cataract_101_train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=N_WORKERS, drop_last=DROP_LAST, pin_memory=True)\n",
    "val_loader_B=DataLoader(cataract_101_val_dataset,batch_size=2,shuffle=False,num_workers=N_WORKERS,drop_last=DROP_LAST)\n",
    "test_loader_A=DataLoader(cadis_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "test_loader_B=DataLoader(cataract_101_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "model_name=\"m2f_cataract101\"\n",
    "val_dataset_name=\"cataract101\"\n",
    "\"\"\"\n",
    "\n",
    "# Option 3: model pretrained on A, finetune on rand(A)+B, test A and B\n",
    "\"\"\"\n",
    "train_loader_replayA_B = DataLoader(replayA_B_train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=N_WORKERS, drop_last=DROP_LAST, pin_memory=True)\n",
    "val_loader_replayA_B=DataLoader(replayA_B_val_dataset,batch_size=2,shuffle=False,num_workers=N_WORKERS,drop_last=DROP_LAST)\n",
    "test_loader_A=DataLoader(cadis_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "test_loader_B=DataLoader(cataract_101_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "model_name=\"m2f_replayCadis+cataract101\"\n",
    "val_dataset_name=\"replayCadis+cataract101\"\n",
    "\"\"\"\n",
    "\n",
    "train_loader=train_loader_A\n",
    "val_loader=val_loader_A\n",
    "test_loaders=[test_loader_A,test_loader_B]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pixel_values', 'pixel_mask'])\n",
      "dict_keys(['pixel_values', 'pixel_mask'])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (12) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/gozdeunver/Desktop/continual-learning/mask2former.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gozdeunver/Desktop/continual-learning/mask2former.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     pixel_masks\u001b[39m.\u001b[39mappend(processed[\u001b[39m\"\u001b[39m\u001b[39mpixel_mask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msqueeze())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gozdeunver/Desktop/continual-learning/mask2former.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m masks \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gozdeunver/Desktop/continual-learning/mask2former.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(pixel_values\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mstack(pixel_values)\u001b[39m.\u001b[39;49mto(device),pixel_mask\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mstack(pixel_masks)\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gozdeunver/Desktop/continual-learning/mask2former.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gozdeunver/Desktop/continual-learning/mask2former.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m loss\u001b[39m=\u001b[39moutputs\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/transformers/models/mask2former/modeling_mask2former.py:2501\u001b[0m, in \u001b[0;36mMask2FormerForUniversalSegmentation.forward\u001b[0;34m(self, pixel_values, mask_labels, class_labels, pixel_mask, output_hidden_states, output_auxiliary_logits, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m   2496\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m   2497\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m   2498\u001b[0m )\n\u001b[1;32m   2499\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 2501\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   2502\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m   2503\u001b[0m     pixel_mask\u001b[39m=\u001b[39;49mpixel_mask,\n\u001b[1;32m   2504\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49muse_auxiliary_loss,\n\u001b[1;32m   2505\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2506\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2507\u001b[0m )\n\u001b[1;32m   2509\u001b[0m loss, loss_dict, auxiliary_logits \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2510\u001b[0m class_queries_logits \u001b[39m=\u001b[39m ()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/transformers/models/mask2former/modeling_mask2former.py:2272\u001b[0m, in \u001b[0;36mMask2FormerModel.forward\u001b[0;34m(self, pixel_values, pixel_mask, output_hidden_states, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m   2269\u001b[0m \u001b[39mif\u001b[39;00m pixel_mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2270\u001b[0m     pixel_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((batch_size, height, width), device\u001b[39m=\u001b[39mpixel_values\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 2272\u001b[0m pixel_level_module_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpixel_level_module(\n\u001b[1;32m   2273\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values, output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states\n\u001b[1;32m   2274\u001b[0m )\n\u001b[1;32m   2276\u001b[0m transformer_module_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_module(\n\u001b[1;32m   2277\u001b[0m     multi_scale_features\u001b[39m=\u001b[39mpixel_level_module_output\u001b[39m.\u001b[39mdecoder_hidden_states,\n\u001b[1;32m   2278\u001b[0m     mask_features\u001b[39m=\u001b[39mpixel_level_module_output\u001b[39m.\u001b[39mdecoder_last_hidden_state,\n\u001b[1;32m   2279\u001b[0m     output_hidden_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2280\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   2281\u001b[0m )\n\u001b[1;32m   2283\u001b[0m encoder_hidden_states \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/transformers/models/mask2former/modeling_mask2former.py:1393\u001b[0m, in \u001b[0;36mMask2FormerPixelLevelModule.forward\u001b[0;34m(self, pixel_values, output_hidden_states)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, pixel_values: Tensor, output_hidden_states: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Mask2FormerPixelLevelModuleOutput:\n\u001b[1;32m   1392\u001b[0m     backbone_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(pixel_values)\u001b[39m.\u001b[39mfeature_maps\n\u001b[0;32m-> 1393\u001b[0m     decoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(backbone_features, output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states)\n\u001b[1;32m   1395\u001b[0m     \u001b[39mreturn\u001b[39;00m Mask2FormerPixelLevelModuleOutput(\n\u001b[1;32m   1396\u001b[0m         encoder_last_hidden_state\u001b[39m=\u001b[39mbackbone_features[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\n\u001b[1;32m   1397\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(backbone_features) \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1398\u001b[0m         decoder_last_hidden_state\u001b[39m=\u001b[39mdecoder_output\u001b[39m.\u001b[39mmask_features,\n\u001b[1;32m   1399\u001b[0m         decoder_hidden_states\u001b[39m=\u001b[39mdecoder_output\u001b[39m.\u001b[39mmulti_scale_features,\n\u001b[1;32m   1400\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/transformers/models/mask2former/modeling_mask2former.py:1317\u001b[0m, in \u001b[0;36mMask2FormerPixelDecoder.forward\u001b[0;34m(self, features, encoder_outputs, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[39m# Send input_embeds_flat + masks_flat + level_pos_embed_flat (backbone + proj layer output) through encoder\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1317\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1318\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minput_embeds_flat,\n\u001b[1;32m   1319\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mmasks_flat,\n\u001b[1;32m   1320\u001b[0m         position_embeddings\u001b[39m=\u001b[39;49mlevel_pos_embed_flat,\n\u001b[1;32m   1321\u001b[0m         spatial_shapes\u001b[39m=\u001b[39;49mspatial_shapes,\n\u001b[1;32m   1322\u001b[0m         level_start_index\u001b[39m=\u001b[39;49mlevel_start_index,\n\u001b[1;32m   1323\u001b[0m         valid_ratios\u001b[39m=\u001b[39;49mvalid_ratios,\n\u001b[1;32m   1324\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1325\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1326\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1327\u001b[0m     )\n\u001b[1;32m   1329\u001b[0m last_hidden_state \u001b[39m=\u001b[39m encoder_outputs\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m   1330\u001b[0m batch_size \u001b[39m=\u001b[39m last_hidden_state\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/transformers/models/mask2former/modeling_mask2former.py:1172\u001b[0m, in \u001b[0;36mMask2FormerPixelDecoderEncoderOnly.forward\u001b[0;34m(self, inputs_embeds, attention_mask, position_embeddings, spatial_shapes, level_start_index, valid_ratios, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m   1170\u001b[0m     all_hidden_states \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (hidden_states\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m),)\n\u001b[0;32m-> 1172\u001b[0m layer_outputs \u001b[39m=\u001b[39m encoder_layer(\n\u001b[1;32m   1173\u001b[0m     hidden_states,\n\u001b[1;32m   1174\u001b[0m     attention_mask,\n\u001b[1;32m   1175\u001b[0m     position_embeddings\u001b[39m=\u001b[39;49mposition_embeddings,\n\u001b[1;32m   1176\u001b[0m     reference_points\u001b[39m=\u001b[39;49mreference_points,\n\u001b[1;32m   1177\u001b[0m     spatial_shapes\u001b[39m=\u001b[39;49mspatial_shapes,\n\u001b[1;32m   1178\u001b[0m     level_start_index\u001b[39m=\u001b[39;49mlevel_start_index,\n\u001b[1;32m   1179\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1180\u001b[0m )\n\u001b[1;32m   1182\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1184\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/transformers/models/mask2former/modeling_mask2former.py:1027\u001b[0m, in \u001b[0;36mMask2FormerPixelDecoderEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_embeddings, reference_points, spatial_shapes, level_start_index, output_attentions)\u001b[0m\n\u001b[1;32m   1024\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m   1026\u001b[0m \u001b[39m# Apply Multi-scale Deformable Attention Module on the multi-scale feature maps.\u001b[39;00m\n\u001b[0;32m-> 1027\u001b[0m hidden_states, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m   1028\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   1029\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1030\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   1031\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1032\u001b[0m     position_embeddings\u001b[39m=\u001b[39;49mposition_embeddings,\n\u001b[1;32m   1033\u001b[0m     reference_points\u001b[39m=\u001b[39;49mreference_points,\n\u001b[1;32m   1034\u001b[0m     spatial_shapes\u001b[39m=\u001b[39;49mspatial_shapes,\n\u001b[1;32m   1035\u001b[0m     level_start_index\u001b[39m=\u001b[39;49mlevel_start_index,\n\u001b[1;32m   1036\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1037\u001b[0m )\n\u001b[1;32m   1039\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m   1040\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/transformers/models/mask2former/modeling_mask2former.py:971\u001b[0m, in \u001b[0;36mMask2FormerPixelDecoderEncoderMultiscaleDeformableAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, position_embeddings, reference_points, spatial_shapes, level_start_index, output_attentions)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    969\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLast dim of reference_points must be 2 or 4, but got \u001b[39m\u001b[39m{\u001b[39;00mreference_points\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 971\u001b[0m output \u001b[39m=\u001b[39m multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n\u001b[1;32m    972\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_proj(output)\n\u001b[1;32m    974\u001b[0m \u001b[39mreturn\u001b[39;00m output, attention_weights\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/transformers/models/mask2former/modeling_mask2former.py:836\u001b[0m, in \u001b[0;36mmulti_scale_deformable_attention\u001b[0;34m(value, value_spatial_shapes, sampling_locations, attention_weights)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[39m# (batch_size, num_queries, num_heads, num_levels, num_points)\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[39m# -> (batch_size, num_heads, num_queries, num_levels, num_points)\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[39m# -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\u001b[39;00m\n\u001b[1;32m    832\u001b[0m attention_weights \u001b[39m=\u001b[39m attention_weights\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m    833\u001b[0m     batch_size \u001b[39m*\u001b[39m num_heads, \u001b[39m1\u001b[39m, num_queries, num_levels \u001b[39m*\u001b[39m num_points\n\u001b[1;32m    834\u001b[0m )\n\u001b[1;32m    835\u001b[0m output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 836\u001b[0m     (torch\u001b[39m.\u001b[39;49mstack(sampling_value_list, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49mflatten(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m) \u001b[39m*\u001b[39;49m attention_weights)\n\u001b[1;32m    837\u001b[0m     \u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    838\u001b[0m     \u001b[39m.\u001b[39mview(batch_size, num_heads \u001b[39m*\u001b[39m hidden_dim, num_queries)\n\u001b[1;32m    839\u001b[0m )\n\u001b[1;32m    840\u001b[0m \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (12) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir=out_dir)\n",
    "\n",
    "best_val_metric=-np.inf\n",
    "\n",
    "model_dir=out_dir+\"models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store weights in: \", model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "best_model_dir=model_dir+f\"{model_name}/best_models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store best model weights in: \", best_model_dir)\n",
    "    os.makedirs(best_model_dir)\n",
    "final_model_dir=model_dir+f\"{model_name}/final_model/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store final model weights in: \", final_model_dir)\n",
    "    os.makedirs(final_model_dir)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        pixel_values=[]\n",
    "        pixel_masks=[]\n",
    "        for image in images:\n",
    "            processed = image_processor(image, return_tensors=\"pt\",do_rescale=False)\n",
    "           \n",
    "            pixel_values.append(processed[\"pixel_values\"].squeeze())\n",
    "            pixel_masks.append(processed[\"pixel_mask\"].squeeze())\n",
    "\n",
    "        masks = masks.to(device)\n",
    "        outputs = model(pixel_values=torch.stack(pixel_values).to(device),pixel_mask=torch.stack(pixel_masks).to(device))\n",
    "        \n",
    "        loss=outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        target_sizes = [(image.shape[0], image.shape[1]) for image in images]\n",
    "        pred_maps = image_processor.post_process_semantic_segmentation(\n",
    "            outputs, target_sizes=target_sizes\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "\n",
    "    train_epoch_miou = metric.compute(num_labels=num_classes, ignore_index=255, reduce_labels=True)['mean_iou']\n",
    "    train_epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {train_epoch_loss:.4f}')\n",
    "    writer.add_scalar(f'Loss/train_{model_name}', train_epoch_loss, epoch+1)\n",
    "    writer.add_scalar(f'mIoU/train_{model_name}', train_epoch_miou, epoch+1)\n",
    "    print(f'mIoU/train_{model_name}:{train_epoch_miou}, Epoch:{epoch+1}')\n",
    "    \n",
    "    # Validation at each 5th epoch\n",
    "    if epoch>0 and epoch %5 ==0:\n",
    "        running_loss_val=0.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_images,val_masks in val_loader:\n",
    "                pixel_values=[]\n",
    "                pixel_masks=[]\n",
    "                val_images = val_images.to(device)\n",
    "                for image in val_images:\n",
    "                    processed = image_processor(image, return_tensors=\"pt\",do_rescale=False)\n",
    "                    \n",
    "                    pixel_values.append(processed[\"pixel_values\"].squeeze())\n",
    "                    pixel_masks.append(processed[\"pixel_mask\"].squeeze())\n",
    "\n",
    "                outputs_val = model(pixel_values=torch.stack(pixel_values).to(device),pixel_mask=torch.stack(pixel_masks).to(device))\n",
    "\n",
    "                val_masks = val_masks.to(device)\n",
    "                val_loss=outputs_val.loss\n",
    "                running_loss_val += val_loss.item() * val_images.size(0)\n",
    "                target_sizes = [(image.shape[0], image.shape[1]) for image in val_images]\n",
    "                pred_maps = image_processor.post_process_semantic_segmentation(outputs_val, target_sizes=target_sizes)\n",
    "                metric.add_batch(references=val_masks, predictions=pred_maps)\n",
    "                \n",
    "        val_epoch_loss = running_loss_val / len(val_loader.dataset)\n",
    "        val_epoch_miou = metric.compute(num_labels=num_classes, ignore_index=255, reduce_labels=True)['mean_iou']\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {val_epoch_loss:.4f}')\n",
    "        writer.add_scalar(f'mIoU/val_{val_dataset_name}', val_epoch_miou, epoch+1)\n",
    "        writer.add_scalar(f'Loss/val_{val_dataset_name}', val_epoch_loss, epoch+1)\n",
    "        print(f'mIoU/val_{model_name}:{val_epoch_miou}, Epoch:{epoch+1}')\n",
    "\n",
    "        if val_epoch_miou>best_val_metric:\n",
    "            best_val_metric=val_epoch_miou\n",
    "            model.save_pretrained(best_model_dir)\n",
    "\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "\n",
    "# Save final model.\n",
    "model.save_pretrained(final_model_dir)\n",
    "print('TRAINING COMPLETE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    # test on cadis test dataset \n",
    "    for test_images,test_masks in test_loader_A:\n",
    "        test_images = test_images.to(device)\n",
    "        \n",
    "        pixel_values=[]\n",
    "        pixel_masks=[]\n",
    "        for image in test_images:\n",
    "            processed = image_processor(image, return_tensors=\"pt\",do_rescale=False)\n",
    "            \n",
    "            pixel_values.append(processed[\"pixel_values\"].squeeze())\n",
    "            pixel_masks.append(processed[\"pixel_mask\"].squeeze())\n",
    "\n",
    "        outputs_test = model(pixel_values=torch.stack(pixel_values).to(device),pixel_mask=torch.stack(pixel_masks).to(device))\n",
    "        \n",
    "        test_masks = test_masks.to(device)\n",
    "        target_sizes = [(image.shape[0], image.shape[1]) for image in test_images]\n",
    "        pred_maps = image_processor.post_process_semantic_segmentation(outputs_test, target_sizes=target_sizes)\n",
    "        metric.add_batch(references=test_masks, predictions=pred_maps)\n",
    "            \n",
    "    test_miou = metric.compute(num_labels=num_classes, ignore_index=255, reduce_labels=True)['mean_iou']\n",
    "\n",
    "    writer.add_scalar(f'mIoU/test_cadis', test_miou)\n",
    "    print(f'mIoU/test_cadis:', test_miou)\n",
    "\n",
    "    # test on cataract101 test dataset\n",
    "    for test_images,test_masks in test_loader_B:\n",
    "        test_images = test_images.to(device)\n",
    "        pixel_values=[]\n",
    "        pixel_masks=[]\n",
    "        for image in test_images:\n",
    "            processed = image_processor(image, return_tensors=\"pt\",do_rescale=False)\n",
    "            \n",
    "            pixel_values.append(processed[\"pixel_values\"].squeeze())\n",
    "            pixel_masks.append(processed[\"pixel_mask\"].squeeze())\n",
    "\n",
    "        outputs_test = model(pixel_values=torch.stack(pixel_values).to(device),pixel_mask=torch.stack(pixel_masks).to(device))\n",
    "        \n",
    "        test_masks = test_masks.to(device)\n",
    "        target_sizes = [(image.shape[0], image.shape[1]) for image in test_images]\n",
    "        pred_maps = image_processor.post_process_semantic_segmentation(outputs_test, target_sizes=target_sizes)\n",
    "        metric.add_batch(references=test_masks, predictions=pred_maps)\n",
    "            \n",
    "    test_miou = metric.compute(num_labels=num_classes, ignore_index=255, reduce_labels=True)['mean_iou']\n",
    "\n",
    "    writer.add_scalar(f'mIoU/test_cataract_101', test_miou)\n",
    "    print(f'mIoU/test_cataract_101:', test_miou)\n",
    "\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "continual-learning-UoKZrllX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

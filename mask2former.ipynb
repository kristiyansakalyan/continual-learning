{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation,Mask2FormerConfig,SwinConfig,Mask2FormerImageProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from utils.dataset_utils import CadisDataset,Cataract101Dataset\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(14180) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using version \u001b[39;1m^0.4.1\u001b[39;22m for \u001b[36mevaluate\u001b[39m\n",
      "\n",
      "\u001b[34mUpdating dependencies\u001b[39m\n",
      "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(13.7s)\u001b[39;22m[34mResolving dependencies...\u001b[39m \u001b[39;2m(1.7s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(1.9s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(3.2s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(4.4s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(6.2s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(6.7s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(10.3s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(10.9s)\u001b[39;22m\n",
      "\n",
      "No dependencies to install or update\n",
      "\n",
      "\u001b[34mWriting lock file\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(14191) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using version \u001b[39;1m^2.16.2\u001b[39;22m for \u001b[36mtensorboard\u001b[39m\n",
      "\n",
      "\u001b[34mUpdating dependencies\u001b[39m\n",
      "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(2.6s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(1.2s)\u001b[39;22m\n",
      "\n",
      "No dependencies to install or update\n",
      "\n",
      "\u001b[34mWriting lock file\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!poetry add evaluate\n",
    "!poetry add tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5d8014a80c28312c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5d8014a80c28312c\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_dir=\"outputs/\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir outputs/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Datasets and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3584, 674, 540, 84, 614, 85)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cataract_101_dataset=Cataract101Dataset(root_folder=\"data/cataract-101\", split=\"train\")\n",
    "total_size=len(cataract_101_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "generator1=torch.Generator().manual_seed(42)\n",
    "cataract_101_train_dataset, cataract_101_val_dataset, cataract_101_test_dataset = random_split(cataract_101_dataset, [train_size, val_size, test_size],generator=generator1)\n",
    "\n",
    "cadis_train_dataset = CadisDataset(root_folder=\"data/cadis\", split=\"train\")\n",
    "cadis_val_dataset = CadisDataset(root_folder=\"data/cadis\", split=\"val\")\n",
    "cadis_test_dataset = CadisDataset(root_folder=\"data/cadis\", split=\"test\")\n",
    "len(cadis_train_dataset),len(cataract_101_train_dataset),len(cadis_val_dataset),len(cataract_101_val_dataset),len(cadis_test_dataset),len(cataract_101_test_dataset)\n",
    "\n",
    "# TODO: merge A+B, rand(A)+B train and val sets: fully_merged_train_dataset, fully_merged_val_dataset, replayA_B_train_dataset, replayA_B_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing The M2F Model, Configs and The Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-ade-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([23]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([23, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([23]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnum_classes=??? # including background\\nimage_processor = AutoImageProcessor.from_pretrained(<location_to_trained_model_dir>,\\n                    ignore_index=255, reduce_labels=True)\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(<location_to_trained_model_dir>,\\n                num_labels=num_classes-1,ignore_mismatched_sizes=True)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ONLY BACKBONE PRETRAINED ON IN22K -> probable baseline\n",
    "\"\"\"\n",
    "config=Mask2FormerConfig()\n",
    "config.use_pretrained_backbone=True\n",
    "config.backbone_config=None\n",
    "config.backbone=\"microsoft/swin-large-patch4-window12-384-in22k\"\n",
    "num_classes=len(cadis_train_dataset.categories) # including background\n",
    "config.num_labels=num_classes-1\n",
    "image_processor=Mask2FormerImageProcessor(ignore_index=255, reduce_labels=True)\n",
    "model=Mask2FormerForUniversalSegmentation(config)\n",
    "\n",
    "\"\"\"\n",
    "# FULL MODEL PRETRAINED ON ADE20K -> We might also use this as baseline\n",
    "\n",
    "num_classes=len(cadis_train_dataset.categories) # including background\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-large-ade-semantic\",\n",
    "                    ignore_index=255, reduce_labels=True)\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-large-ade-semantic\",\n",
    "                num_labels=num_classes-1,ignore_mismatched_sizes=True)\n",
    "\n",
    "# MODEL PREVIOUSLY TRAINED ON ONE OF OUR DATASETS\n",
    "\"\"\"\n",
    "num_classes=??? # including background\n",
    "image_processor = AutoImageProcessor.from_pretrained(<location_to_trained_model_dir>,\n",
    "                    ignore_index=255, reduce_labels=True)\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(<location_to_trained_model_dir>,\n",
    "                num_labels=num_classes-1,ignore_mismatched_sizes=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "learning_rate_multiplier=0.1\n",
    "backbone_lr=LEARNING_RATE*learning_rate_multiplier\n",
    "weight_decay=0.5\n",
    "#dice = Dice(average='micro')\n",
    "\n",
    "#lambda_CE=5.0\n",
    "#lambda_dice=5.0\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "encoder_params=[param for name, param in model.named_parameters() if name.startswith(\"model.pixel_level_module.encoder\")]\n",
    "decoder_params=[param for name, param in model.named_parameters() if name.startswith(\"model.pixel_level_module.decoder\")]\n",
    "transformer_params=[param for name, param in model.named_parameters() if name.startswith(\"model.transformer_module\")]\n",
    "optimizer = optim.AdamW([{'params': encoder_params, 'lr': backbone_lr},\\\n",
    "                         {'params': decoder_params}, \\\n",
    "                            {'params':transformer_params}], \\\n",
    "                                lr=LEARNING_RATE,weight_decay=weight_decay)\n",
    "\n",
    "scheduler = optim.lr_scheduler.PolynomialLR(optimizer,total_iters=NUM_EPOCHS,power=0.9)\n",
    "\"\"\"\n",
    "or \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = MultiStepLR(\n",
    "        optimizer, milestones=50, gamma=0.1, verbose=True\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "#CE_weight = torch.ones(num_classes)*2.0\n",
    "#CE_weight[0] = 0.1\n",
    "\n",
    "# Dataloading\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 16\n",
    "SHUFFLE = True\n",
    "DROP_LAST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: model pretrained on ADE20K, finetune on only A, test A and B\n",
    "train_loader_A = DataLoader(cadis_train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=N_WORKERS, drop_last=DROP_LAST, pin_memory=True)\n",
    "val_loader_A=DataLoader(cadis_val_dataset,batch_size=2,shuffle=False,num_workers=N_WORKERS,drop_last=DROP_LAST)\n",
    "test_loader_A=DataLoader(cadis_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "test_loader_B=DataLoader(cataract_101_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "model_name=\"m2f_cadis\"\n",
    "val_dataset_name=\"cadis\"\n",
    "\n",
    "# Option 2: model pretrained on A, finetune on  A+B, test A and B\n",
    "\"\"\"\n",
    "train_loader_fully_merged = DataLoader(fully_merged_train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=N_WORKERS, drop_last=DROP_LAST, pin_memory=True)\n",
    "val_loader_fully_merged=DataLoader(fully_merged_val_dataset,batch_size=2,shuffle=False,num_workers=N_WORKERS,drop_last=DROP_LAST)\n",
    "test_loader_A=DataLoader(cadis_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "test_loader_B=DataLoader(cataract_101_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "model_name=\"m2f_cadis+cataract101\"\n",
    "val_dataset_name=\"cadis+cataract101\"\n",
    "\"\"\"\n",
    "\n",
    "# Option 3: model pretrained on A, finetune on B, test A and B\n",
    "\"\"\"\n",
    "train_loader_B = DataLoader(cataract_101_train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=N_WORKERS, drop_last=DROP_LAST, pin_memory=True)\n",
    "val_loader_B=DataLoader(cataract_101_val_dataset,batch_size=2,shuffle=False,num_workers=N_WORKERS,drop_last=DROP_LAST)\n",
    "test_loader_A=DataLoader(cadis_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "test_loader_B=DataLoader(cataract_101_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "model_name=\"m2f_cataract101\"\n",
    "val_dataset_name=\"cataract101\"\n",
    "\"\"\"\n",
    "\n",
    "# Option 3: model pretrained on A, finetune on rand(A)+B, test A and B\n",
    "\"\"\"\n",
    "train_loader_replayA_B = DataLoader(replayA_B_train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=N_WORKERS, drop_last=DROP_LAST, pin_memory=True)\n",
    "val_loader_replayA_B=DataLoader(replayA_B_val_dataset,batch_size=2,shuffle=False,num_workers=N_WORKERS,drop_last=DROP_LAST)\n",
    "test_loader_A=DataLoader(cadis_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "test_loader_B=DataLoader(cataract_101_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "model_name=\"m2f_replayCadis+cataract101\"\n",
    "val_dataset_name=\"replayCadis+cataract101\"\n",
    "\"\"\"\n",
    "\n",
    "train_loader=train_loader_A\n",
    "val_loader=val_loader_A\n",
    "test_loaders=[test_loader_A,test_loader_B]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 3024, 12]) torch.Size([16, 1, 3024, 12]) torch.Size([16, 32, 3024]) 2 256 3024\n",
      "torch.Size([16, 32, 3024, 12]) torch.Size([16, 1, 3024, 12]) torch.Size([16, 32, 3024]) 2 256 3024\n",
      "torch.Size([16, 32, 3024, 12]) torch.Size([16, 1, 3024, 12]) torch.Size([16, 32, 3024]) 2 256 3024\n",
      "torch.Size([16, 32, 3024, 12]) torch.Size([16, 1, 3024, 12]) torch.Size([16, 32, 3024]) 2 256 3024\n",
      "torch.Size([16, 32, 3024, 12]) torch.Size([16, 1, 3024, 12]) torch.Size([16, 32, 3024]) 2 256 3024\n",
      "torch.Size([16, 32, 3024, 12]) torch.Size([16, 1, 3024, 12]) torch.Size([16, 32, 3024]) 2 256 3024\n",
      "TRAINING COMPLETE\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir=out_dir)\n",
    "\n",
    "best_val_metric=-np.inf\n",
    "\n",
    "model_dir=out_dir+\"models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store weights in: \", model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "best_model_dir=model_dir+f\"{model_name}/best_models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store best model weights in: \", best_model_dir)\n",
    "    os.makedirs(best_model_dir)\n",
    "final_model_dir=model_dir+f\"{model_name}/final_model/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store final model weights in: \", final_model_dir)\n",
    "    os.makedirs(final_model_dir)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        pixel_values=[]\n",
    "        pixel_masks=[]\n",
    "        mask_labels=[]\n",
    "        class_labels=[]\n",
    "        for i in range(len(images)):\n",
    "            image=images[i]\n",
    "            mask=masks[i]\n",
    "            processed = image_processor(image, mask,return_tensors=\"pt\",do_rescale=False)\n",
    "           \n",
    "            pixel_values.append(processed[\"pixel_values\"].squeeze())\n",
    "            pixel_masks.append(processed[\"pixel_mask\"].squeeze())\n",
    "            mask_labels.append(processed[\"mask_labels\"][0].to(device))\n",
    "            class_labels.append(processed[\"class_labels\"][0].to(device))\n",
    "\n",
    "        \n",
    "        pixel_values=torch.stack(pixel_values).to(device)\n",
    "        pixel_mask=torch.stack(pixel_masks).to(device)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values,pixel_mask=pixel_mask,class_labels=class_labels,mask_labels=mask_labels)\n",
    "      \n",
    "        loss=outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        target_sizes = [(image.shape[0], image.shape[1]) for image in images]\n",
    "        pred_maps = image_processor.post_process_semantic_segmentation(\n",
    "            outputs, target_sizes=target_sizes\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "    \n",
    "    train_epoch_miou = metric.compute(num_labels=num_classes, ignore_index=255, reduce_labels=True)['mean_iou']\n",
    "    train_epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {train_epoch_loss:.4f}')\n",
    "    writer.add_scalar(f'Loss/train_{model_name}', train_epoch_loss, epoch+1)\n",
    "    writer.add_scalar(f'mIoU/train_{model_name}', train_epoch_miou, epoch+1)\n",
    "    print(f'mIoU/train_{model_name}:{train_epoch_miou}, Epoch:{epoch+1}')\n",
    "    \n",
    "    # Validation at each 5th epoch\n",
    "    if epoch>0 and epoch %5 ==0:\n",
    "        running_loss_val=0.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_images,val_masks in val_loader:\n",
    "                pixel_values=[]\n",
    "                pixel_masks=[]\n",
    "                mask_labels=[]\n",
    "                class_labels=[]\n",
    "                val_images = val_images.to(device)\n",
    "                val_masks = val_masks.to(device)\n",
    "                for i in range(len(val_images)):\n",
    "                    image=val_images[i]\n",
    "                    mask=val_masks[i]\n",
    "                    processed = image_processor(image, mask,return_tensors=\"pt\",do_rescale=False)\n",
    "                    \n",
    "                    pixel_values.append(processed[\"pixel_values\"].squeeze())\n",
    "                    pixel_masks.append(processed[\"pixel_mask\"].squeeze())\n",
    "                    mask_labels.append(processed[\"mask_labels\"][0].to(device))\n",
    "                    class_labels.append(processed[\"class_labels\"][0].to(device))\n",
    "\n",
    "                pixel_values=torch.stack(pixel_values).to(device)\n",
    "                pixel_mask=torch.stack(pixel_masks).to(device)\n",
    "\n",
    "                outputs_val = model(pixel_values=pixel_values,pixel_mask=pixel_mask,class_labels=class_labels,mask_labels=mask_labels)\n",
    "                val_loss=outputs_val.loss\n",
    "                running_loss_val += val_loss.item() * val_images.size(0)\n",
    "                target_sizes = [(image.shape[0], image.shape[1]) for image in val_images]\n",
    "                pred_maps = image_processor.post_process_semantic_segmentation(outputs_val, target_sizes=target_sizes)\n",
    "                metric.add_batch(references=val_masks, predictions=pred_maps)\n",
    "                \n",
    "        val_epoch_loss = running_loss_val / len(val_loader.dataset)\n",
    "        val_epoch_miou = metric.compute(num_labels=num_classes, ignore_index=255, reduce_labels=True)['mean_iou']\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {val_epoch_loss:.4f}')\n",
    "        writer.add_scalar(f'mIoU/val_{val_dataset_name}', val_epoch_miou, epoch+1)\n",
    "        writer.add_scalar(f'Loss/val_{val_dataset_name}', val_epoch_loss, epoch+1)\n",
    "        print(f'mIoU/val_{model_name}:{val_epoch_miou}, Epoch:{epoch+1}')\n",
    "\n",
    "        if val_epoch_miou>best_val_metric:\n",
    "            best_val_metric=val_epoch_miou\n",
    "            model.save_pretrained(best_model_dir)\n",
    "\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "\n",
    "# Save final model.\n",
    "model.save_pretrained(final_model_dir)\n",
    "print('TRAINING COMPLETE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    # test on cadis test dataset \n",
    "    for test_images,test_masks in test_loader_A:\n",
    "        test_images = test_images.to(device)\n",
    "        test_masks = test_masks.to(device)\n",
    "        pixel_values=[]\n",
    "        pixel_masks=[]\n",
    "        mask_labels=[]\n",
    "        class_labels=[]\n",
    "        for i in range(len(test_images)):\n",
    "            image=test_images[i]\n",
    "            mask=test_masks[i]\n",
    "            processed = image_processor(image, return_tensors=\"pt\",do_rescale=False)\n",
    "            \n",
    "            pixel_values.append(processed[\"pixel_values\"].squeeze())\n",
    "            pixel_masks.append(processed[\"pixel_mask\"].squeeze())\n",
    "            mask_labels.append(processed[\"mask_labels\"][0].to(device))\n",
    "            class_labels.append(processed[\"class_labels\"][0].to(device))\n",
    "        \n",
    "        pixel_values=torch.stack(pixel_values).to(device)\n",
    "        pixel_mask=torch.stack(pixel_masks).to(device)\n",
    "\n",
    "        outputs_test = model(pixel_values=pixel_values,pixel_mask=pixel_mask,class_labels=class_labels,mask_labels=mask_labels)\n",
    "    \n",
    "        target_sizes = [(image.shape[0], image.shape[1]) for image in test_images]\n",
    "        pred_maps = image_processor.post_process_semantic_segmentation(outputs_test, target_sizes=target_sizes)\n",
    "        metric.add_batch(references=test_masks, predictions=pred_maps)\n",
    "            \n",
    "    test_miou = metric.compute(num_labels=num_classes, ignore_index=255, reduce_labels=True)['mean_iou']\n",
    "\n",
    "    writer.add_scalar(f'mIoU/test_cadis', test_miou)\n",
    "    print(f'mIoU/test_cadis:', test_miou)\n",
    "\n",
    "    # test on cataract101 test dataset\n",
    "    for test_images,test_masks in test_loader_B:\n",
    "        test_images = test_images.to(device)\n",
    "        test_masks = test_masks.to(device)\n",
    "        pixel_values=[]\n",
    "        pixel_masks=[]\n",
    "        mask_labels=[]\n",
    "        class_labels=[]\n",
    "        for i in range(len(test_images)):\n",
    "            image=test_images[i]\n",
    "            mask=test_masks[i]\n",
    "            processed = image_processor(image, return_tensors=\"pt\",do_rescale=False)\n",
    "            \n",
    "            pixel_values.append(processed[\"pixel_values\"].squeeze())\n",
    "            pixel_masks.append(processed[\"pixel_mask\"].squeeze())\n",
    "            mask_labels.append(processed[\"mask_labels\"][0].to(device))\n",
    "            class_labels.append(processed[\"class_labels\"][0].to(device))\n",
    "        \n",
    "        pixel_values=torch.stack(pixel_values).to(device)\n",
    "        pixel_mask=torch.stack(pixel_masks).to(device)\n",
    "\n",
    "        outputs_test = model(pixel_values=pixel_values,pixel_mask=pixel_mask,class_labels=class_labels,mask_labels=mask_labels)\n",
    "        target_sizes = [(image.shape[0], image.shape[1]) for image in test_images]\n",
    "        pred_maps = image_processor.post_process_semantic_segmentation(outputs_test, target_sizes=target_sizes)\n",
    "        metric.add_batch(references=test_masks, predictions=pred_maps)\n",
    "            \n",
    "    test_miou = metric.compute(num_labels=num_classes, ignore_index=255, reduce_labels=True)['mean_iou']\n",
    "\n",
    "    writer.add_scalar(f'mIoU/test_cataract_101', test_miou)\n",
    "    print(f'mIoU/test_cataract_101:', test_miou)\n",
    "\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "continual-learning-UoKZrllX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

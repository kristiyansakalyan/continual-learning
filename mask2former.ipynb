{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from utils.dataset_utils import CadisDataset,Cataract101Dataset\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 2817), started 0:08:32 ago. (Use '!kill 2817' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6668f6b157ec29b1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6668f6b157ec29b1\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_dir=\"outputs/\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{out_dir}'runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Datasets and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3584, 674, 540, 84, 614, 85)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cataract_101_dataset=Cataract101Dataset(root_folder=\"data/cataract-101\", split=\"train\")\n",
    "total_size=len(cataract_101_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "generator1=torch.Generator().manual_seed(42)\n",
    "cataract_101_train_dataset, cataract_101_val_dataset, cataract_101_test_dataset = random_split(cataract_101_dataset, [train_size, val_size, test_size],generator=generator1)\n",
    "\n",
    "cadis_train_dataset = CadisDataset(root_folder=\"data/cadis\", split=\"train\")\n",
    "cadis_val_dataset = CadisDataset(root_folder=\"data/cadis\", split=\"val\")\n",
    "cadis_test_dataset = CadisDataset(root_folder=\"data/cadis\", split=\"test\")\n",
    "len(cadis_train_dataset),len(cataract_101_train_dataset),len(cadis_val_dataset),len(cataract_101_val_dataset),len(cadis_test_dataset),len(cataract_101_test_dataset)\n",
    "\n",
    "# TODO: merge A+B, rand(A)+B train and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing The M2F Model, Configs and The Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-ade-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([23]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([23, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([23]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_classes=len(cadis_train_dataset.categories) # including background\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-large-ade-semantic\",ignore_index=255, reduce_labels=True)\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-large-ade-semantic\",num_labels=num_classes-1,ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "learning_rate_multiplier=0.1\n",
    "backbone_lr=LEARNING_RATE*learning_rate_multiplier\n",
    "weight_decay=0.5\n",
    "#dice = Dice(average='micro')\n",
    "\n",
    "#lambda_CE=5.0\n",
    "#lambda_dice=5.0\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "encoder_params=[param for name, param in model.named_parameters() if name.startswith(\"model.pixel_level_module.encoder\")]\n",
    "decoder_params=[param for name, param in model.named_parameters() if name.startswith(\"model.pixel_level_module.decoder\")]\n",
    "transformer_params=[param for name, param in model.named_parameters() if name.startswith(\"model.transformer_module\")]\n",
    "optimizer = optim.AdamW([{'params': encoder_params, 'lr': backbone_lr},\\\n",
    "                         {'params': decoder_params}, \\\n",
    "                            {'params':transformer_params}], \\\n",
    "                                lr=LEARNING_RATE,weight_decay=weight_decay)\n",
    "\n",
    "scheduler = optim.lr_scheduler.PolynomialLR(optimizer,total_iters=NUM_EPOCHS,power=0.9)\n",
    "\"\"\"\n",
    "or \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = MultiStepLR(\n",
    "        optimizer, milestones=50, gamma=0.1, verbose=True\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "#CE_weight = torch.ones(num_classes)*2.0\n",
    "#CE_weight[0] = 0.1\n",
    "\n",
    "# Dataloading\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 16\n",
    "SHUFFLE = True\n",
    "DROP_LAST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: model pretrained on ADE20K, finetune on only A, test A and B\n",
    "train_loader_A = DataLoader(cadis_train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=N_WORKERS, drop_last=DROP_LAST, pin_memory=True)\n",
    "val_loader_A=DataLoader(cadis_val_dataset,batch_size=2,shuffle=False,num_workers=N_WORKERS,drop_last=DROP_LAST)\n",
    "test_loader_A=DataLoader(cadis_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "test_loader_B=DataLoader(cataract_101_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "model_name=\"m2f_cadis\"\n",
    "val_dataset_name=\"cadis\"\n",
    "\n",
    "# Option 2: model pretrained on A, finetune on  A+B, test A and B\n",
    "\"\"\"\n",
    "train_loader_fully_merged = DataLoader(fully_merged_train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=N_WORKERS, drop_last=DROP_LAST, pin_memory=True)\n",
    "val_loader_fully_merged=DataLoader(fully_merged_val_dataset,batch_size=2,shuffle=False,num_workers=N_WORKERS,drop_last=DROP_LAST)\n",
    "test_loader_A=DataLoader(cadis_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "test_loader_B=DataLoader(cataract_101_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "model_name=\"m2f_cadis+cataract101\"\n",
    "val_dataset_name=\"cadis+cataract101\"\n",
    "\"\"\"\n",
    "\n",
    "# Option 3: model pretrained on A, finetune on B, test A and B\n",
    "\"\"\"\n",
    "train_loader_B = DataLoader(cataract_101_train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=N_WORKERS, drop_last=DROP_LAST, pin_memory=True)\n",
    "val_loader_B=DataLoader(cataract_101_val_dataset,batch_size=2,shuffle=False,num_workers=N_WORKERS,drop_last=DROP_LAST)\n",
    "test_loader_A=DataLoader(cadis_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "test_loader_B=DataLoader(cataract_101_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "model_name=\"m2f_cataract101\"\n",
    "val_dataset_name=\"cataract101\"\n",
    "\"\"\"\n",
    "\n",
    "# Option 3: model pretrained on A, finetune on rand(A)+B, test A and B\n",
    "\"\"\"\n",
    "train_loader_replayA_B = DataLoader(replayA_B_train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=N_WORKERS, drop_last=DROP_LAST, pin_memory=True)\n",
    "val_loader_replayA_B=DataLoader(replayA_B_val_dataset,batch_size=2,shuffle=False,num_workers=N_WORKERS,drop_last=DROP_LAST)\n",
    "test_loader_A=DataLoader(cadis_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "test_loader_B=DataLoader(cataract_101_test_dataset,batch_size=1,shuffle=False,num_workers=N_WORKERS,drop_last=False)\n",
    "model_name=\"m2f_replayCadis+cataract101\"\n",
    "val_dataset_name=\"replayCadis+cataract101\"\n",
    "\"\"\"\n",
    "\n",
    "train_loader=train_loader_A\n",
    "val_loader=val_loader_A\n",
    "test_loaders=[test_loader_A,test_loader_B]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=out_dir)\n",
    "\n",
    "best_val_metric=-np.inf\n",
    "\n",
    "model_dir=out_dir+\"models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store weights in: \", model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "best_model_dir=model_dir+f\"{model_name}/best_models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store best model weights in: \", best_model_dir)\n",
    "    os.makedirs(best_model_dir)\n",
    "final_model_dir=model_dir+f\"{model_name}/final_model/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store final model weights in: \", final_model_dir)\n",
    "    os.makedirs(final_model_dir)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        inputs = image_processor(images, return_tensors=\"pt\",do_rescale=False)\n",
    "        masks = masks.to(device)\n",
    "        outputs = model(**inputs)\n",
    "        loss=outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        target_sizes = [(image.shape[0], image.shape[1]) for image in images]\n",
    "        pred_maps = image_processor.post_process_semantic_segmentation(\n",
    "            outputs, target_sizes=target_sizes\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "\n",
    "    train_epoch_miou = metric.compute(num_labels=num_classes, ignore_index=255, reduce_labels=True)['mean_iou']\n",
    "    train_epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {train_epoch_loss:.4f}')\n",
    "    writer.add_scalar(f'Loss/train_{model_name}', train_epoch_loss, epoch+1)\n",
    "    writer.add_scalar(f'mIoU/train_{model_name}', train_epoch_miou, epoch+1)\n",
    "    print(f'mIoU/train_{model_name}:{train_epoch_miou}, Epoch:{epoch+1}')\n",
    "\n",
    "    if epoch>0 and epoch %5 ==0:\n",
    "        running_loss_val=0.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_images,val_masks in val_loader:\n",
    "                val_images = val_images.to(device)\n",
    "                val_inputs = image_processor(val_images, return_tensors=\"pt\",do_rescale=False)\n",
    "                val_masks = val_masks.to(device)\n",
    "                outputs_val = model(**val_inputs)\n",
    "                val_loss=outputs_val.loss\n",
    "                running_loss_val += val_loss.item() * val_images.size(0)\n",
    "                target_sizes = [(image.shape[0], image.shape[1]) for image in val_images]\n",
    "                pred_maps = image_processor.post_process_semantic_segmentation(outputs_val, target_sizes=target_sizes)\n",
    "                metric.add_batch(references=val_masks, predictions=pred_maps)\n",
    "                \n",
    "        val_epoch_loss = running_loss_val / len(val_loader.dataset)\n",
    "        val_epoch_miou = metric.compute(num_labels=num_classes, ignore_index=255, reduce_labels=True)['mean_iou']\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {val_epoch_loss:.4f}')\n",
    "        writer.add_scalar(f'mIoU/val_{val_dataset_name}', val_epoch_miou, epoch+1)\n",
    "        writer.add_scalar(f'Loss/val_{val_dataset_name}', val_epoch_loss, epoch+1)\n",
    "        print(f'mIoU/val_{model_name}:{val_epoch_miou}, Epoch:{epoch+1}')\n",
    "\n",
    "        if val_epoch_miou>best_val_metric:\n",
    "            best_val_metric=val_epoch_miou\n",
    "            model.save_pretrained(best_model_dir)\n",
    "\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "\n",
    "# Save final model.\n",
    "model.save_pretrained(final_model_dir)\n",
    "print('TRAINING COMPLETE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for test_loader in test_loaders:\n",
    "        \n",
    "        # TODO: test dataset names to store them in tensorboard\n",
    "\n",
    "        for test_images,test_masks in test_loader:\n",
    "            test_images = test_images.to(device)\n",
    "            test_inputs = image_processor(test_images, return_tensors=\"pt\",do_rescale=False)\n",
    "            test_masks = test_masks.to(device)\n",
    "            outputs_test = model(**test_inputs)\n",
    "            target_sizes = [(image.shape[0], image.shape[1]) for image in test_images]\n",
    "            pred_maps = image_processor.post_process_semantic_segmentation(outputs_test, target_sizes=target_sizes)\n",
    "            metric.add_batch(references=test_masks, predictions=pred_maps)\n",
    "                \n",
    "        test_miou = metric.compute(num_labels=num_classes, ignore_index=255, reduce_labels=True)['mean_iou']\n",
    "\n",
    "        writer.add_scalar(f'mIoU/test_{test_dataset_name}', test_miou)\n",
    "        print(f'mIoU/test_{test_dataset_name}:', test_miou)\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(683, 512)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = (\n",
    "    \"https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\"\n",
    ")\n",
    "image=Image.open(requests.get(url, stream=True).raw)\n",
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3373, 0.3333, 0.3333,  ..., 0.3059, 0.3059, 0.3059],\n",
       "         [0.3373, 0.3333, 0.3333,  ..., 0.3059, 0.3059, 0.3059],\n",
       "         [0.3373, 0.3333, 0.3333,  ..., 0.3059, 0.3059, 0.3059],\n",
       "         ...,\n",
       "         [0.3490, 0.3608, 0.3608,  ..., 0.4471, 0.4471, 0.4510],\n",
       "         [0.3686, 0.3725, 0.3686,  ..., 0.4588, 0.4627, 0.4667],\n",
       "         [0.3686, 0.3686, 0.3569,  ..., 0.4588, 0.4627, 0.4667]],\n",
       "\n",
       "        [[0.5412, 0.5373, 0.5373,  ..., 0.5059, 0.5059, 0.5059],\n",
       "         [0.5412, 0.5373, 0.5373,  ..., 0.5059, 0.5059, 0.5059],\n",
       "         [0.5412, 0.5373, 0.5373,  ..., 0.5059, 0.5059, 0.5059],\n",
       "         ...,\n",
       "         [0.3765, 0.3882, 0.3882,  ..., 0.4353, 0.4353, 0.4392],\n",
       "         [0.3922, 0.3961, 0.3922,  ..., 0.4471, 0.4510, 0.4549],\n",
       "         [0.3922, 0.3922, 0.3804,  ..., 0.4471, 0.4510, 0.4549]],\n",
       "\n",
       "        [[0.7294, 0.7255, 0.7333,  ..., 0.7294, 0.7294, 0.7294],\n",
       "         [0.7294, 0.7255, 0.7333,  ..., 0.7294, 0.7294, 0.7294],\n",
       "         [0.7294, 0.7255, 0.7333,  ..., 0.7294, 0.7294, 0.7294],\n",
       "         ...,\n",
       "         [0.2157, 0.2275, 0.2275,  ..., 0.4078, 0.4078, 0.4039],\n",
       "         [0.2431, 0.2471, 0.2431,  ..., 0.4196, 0.4235, 0.4196],\n",
       "         [0.2431, 0.2431, 0.2314,  ..., 0.4196, 0.4235, 0.4196]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torchvision import transforms\n",
    "transform=transforms.Compose([transforms.ToTensor()])\n",
    "image=transform(image)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "continual-learning-UoKZrllX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
